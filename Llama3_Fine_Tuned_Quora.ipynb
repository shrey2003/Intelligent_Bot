{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9056575,"sourceType":"datasetVersion","datasetId":5460913},{"sourceId":9060910,"sourceType":"datasetVersion","datasetId":5464195},{"sourceId":33551,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":28083,"modelId":39106}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-29T23:59:23.560409Z","iopub.execute_input":"2024-07-29T23:59:23.561212Z","iopub.status.idle":"2024-07-29T23:59:23.935406Z","shell.execute_reply.started":"2024-07-29T23:59:23.561176Z","shell.execute_reply":"2024-07-29T23:59:23.934550Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/quora-dataset-processed/quora_dataset.csv\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model.safetensors.index.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00003-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/config.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/LICENSE\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00001-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/USE_POLICY.md\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/tokenizer.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/tokenizer_config.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/example_text_completion.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/test_tokenizer.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/requirements.txt\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/tokenizer.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00004-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/eval_details.md\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/special_tokens_map.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/generation.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00002-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/__init__.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/example_chat_completion.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/setup.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/generation_config.json\n/kaggle/input/quora-dataset/quora_dataset_test.csv\n/kaggle/input/quora-dataset/quora_dataset_train.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"%%capture\n%pip install -U transformers \n%pip install -U datasets \n%pip install -U accelerate \n%pip install -U peft \n%pip install -U trl \n%pip install -U bitsandbytes \n%pip install -U wandb","metadata":{"execution":{"iopub.status.busy":"2024-07-29T23:59:23.937031Z","iopub.execute_input":"2024-07-29T23:59:23.937418Z","iopub.status.idle":"2024-07-30T00:01:22.159431Z","shell.execute_reply.started":"2024-07-29T23:59:23.937392Z","shell.execute_reply":"2024-07-30T00:01:22.158156Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format","metadata":{"execution":{"iopub.status.busy":"2024-07-30T00:01:22.160881Z","iopub.execute_input":"2024-07-30T00:01:22.161163Z","iopub.status.idle":"2024-07-30T00:01:40.271624Z","shell.execute_reply.started":"2024-07-30T00:01:22.161136Z","shell.execute_reply":"2024-07-30T00:01:40.270812Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-07-30 00:01:29.023970: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-30 00:01:29.024082: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-30 00:01:29.159111: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# Load the train and test data\ndf = pd.read_csv('/kaggle/input/quora-dataset-processed/quora_dataset.csv')\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\ntest_df=test_df[:250]\ntrain_df=train_df[:4000]\n\ntrain_df['question'] = train_df['question'].astype(str)\ntrain_df['answer'] = train_df['answer'].astype(str)\ntest_df['question'] = test_df['question'].astype(str)\ntest_df['answer'] = test_df['answer'].astype(str)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T00:01:40.272733Z","iopub.execute_input":"2024-07-30T00:01:40.273348Z","iopub.status.idle":"2024-07-30T00:01:41.432011Z","shell.execute_reply.started":"2024-07-30T00:01:40.273319Z","shell.execute_reply":"2024-07-30T00:01:41.431222Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\nfrom datasets import load_dataset\nfrom transformers import LlamaTokenizer, LlamaForSequenceClassification, Trainer, TrainingArguments\n\n# Convert pandas DataFrames to datasets.Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T00:01:41.435038Z","iopub.execute_input":"2024-07-30T00:01:41.435858Z","iopub.status.idle":"2024-07-30T00:01:41.501766Z","shell.execute_reply.started":"2024-07-30T00:01:41.435822Z","shell.execute_reply":"2024-07-30T00:01:41.501011Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n\nlogin(token = hf_token)\n\nwb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune Llama 3 8B on Quora_Qs_Ans', \n    job_type=\"training\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T00:01:41.502847Z","iopub.execute_input":"2024-07-30T00:01:41.503120Z","iopub.status.idle":"2024-07-30T00:02:03.941834Z","shell.execute_reply.started":"2024-07-30T00:01:41.503096Z","shell.execute_reply":"2024-07-30T00:02:03.940906Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfeluda0307\u001b[0m (\u001b[33mfeluda0307-gojo-squad\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240730_000146-na42meu4</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/feluda0307-gojo-squad/Fine-tune%20Llama%203%208B%20on%20Quora_Qs_Ans/runs/na42meu4' target=\"_blank\">absurd-donkey-17</a></strong> to <a href='https://wandb.ai/feluda0307-gojo-squad/Fine-tune%20Llama%203%208B%20on%20Quora_Qs_Ans' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/feluda0307-gojo-squad/Fine-tune%20Llama%203%208B%20on%20Quora_Qs_Ans' target=\"_blank\">https://wandb.ai/feluda0307-gojo-squad/Fine-tune%20Llama%203%208B%20on%20Quora_Qs_Ans</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/feluda0307-gojo-squad/Fine-tune%20Llama%203%208B%20on%20Quora_Qs_Ans/runs/na42meu4' target=\"_blank\">https://wandb.ai/feluda0307-gojo-squad/Fine-tune%20Llama%203%208B%20on%20Quora_Qs_Ans/runs/na42meu4</a>"},"metadata":{}}]},{"cell_type":"code","source":"base_model = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\nnew_model = \"llama-3-8b-fine-tuned-Quora\"","metadata":{"execution":{"iopub.status.busy":"2024-07-30T00:02:03.942970Z","iopub.execute_input":"2024-07-30T00:02:03.943773Z","iopub.status.idle":"2024-07-30T00:02:03.948378Z","shell.execute_reply.started":"2024-07-30T00:02:03.943745Z","shell.execute_reply":"2024-07-30T00:02:03.947252Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"torch_dtype = torch.float16\nattn_implementation = \"eager\"","metadata":{"execution":{"iopub.status.busy":"2024-07-30T00:02:03.949583Z","iopub.execute_input":"2024-07-30T00:02:03.949839Z","iopub.status.idle":"2024-07-30T00:02:03.958377Z","shell.execute_reply.started":"2024-07-30T00:02:03.949815Z","shell.execute_reply":"2024-07-30T00:02:03.957562Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T00:02:03.959572Z","iopub.execute_input":"2024-07-30T00:02:03.959882Z","iopub.status.idle":"2024-07-30T00:04:19.619961Z","shell.execute_reply.started":"2024-07-30T00:02:03.959859Z","shell.execute_reply":"2024-07-30T00:04:19.618798Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"000fdcda6b174e4da102542da4276681"}},"metadata":{}}]},{"cell_type":"code","source":"# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model)\nmodel, tokenizer = setup_chat_format(model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T00:04:19.621401Z","iopub.execute_input":"2024-07-30T00:04:19.621792Z","iopub.status.idle":"2024-07-30T00:04:20.171611Z","shell.execute_reply.started":"2024-07-30T00:04:19.621754Z","shell.execute_reply":"2024-07-30T00:04:20.170418Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T00:04:20.172750Z","iopub.execute_input":"2024-07-30T00:04:20.173036Z","iopub.status.idle":"2024-07-30T00:04:21.038166Z","shell.execute_reply.started":"2024-07-30T00:04:20.173009Z","shell.execute_reply":"2024-07-30T00:04:21.037222Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Define the function to format chat template\ndef format_chat_template(row):\n    user_content = row[\"question\"] if row[\"question\"] is not None else \"\"\n    assistant_content = row[\"answer\"] if row[\"answer\"] is not None else \"\"\n    \n    row_json = [{\"role\": \"user\", \"content\": user_content},\n                {\"role\": \"assistant\", \"content\": assistant_content}]\n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\n# Apply the function to train and test datasets\ntrain_dataset = train_dataset.map(format_chat_template, num_proc=4)\ntest_dataset = test_dataset.map(format_chat_template, num_proc=4)\n\n# Tokenize the datasets\ndef preprocess_function(examples):\n    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)\n\ntrain_dataset = train_dataset.map(preprocess_function, batched=True)\ntest_dataset = test_dataset.map(preprocess_function, batched=True)\n\n# Set the format for PyTorch\ntrain_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\ntest_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n\n# Print an example\ntrain_dataset['text'][3]\ntest_dataset['text'][3]\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T00:04:21.039116Z","iopub.execute_input":"2024-07-30T00:04:21.039413Z","iopub.status.idle":"2024-07-30T00:04:26.105583Z","shell.execute_reply.started":"2024-07-30T00:04:21.039388Z","shell.execute_reply":"2024-07-30T00:04:26.104522Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/4000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df7966ef00024937aa3db736861c0dd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ccb6a72ac3e4af49a6e43bf938256f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d683bdabd69d4f0b98e74489a5dac1a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1328ea26c3cc496e9bd7464b33b2d519"}},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'<|im_start|>user\\nwhat deal v us navy abbreviations eg vfa val vaq<|im_end|>\\n<|im_start|>assistant\\nv french word volare loosely translated means winged aircraft<|im_end|>\\n'"},"metadata":{}}]},{"cell_type":"code","source":"pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-07-30T00:04:26.107190Z","iopub.execute_input":"2024-07-30T00:04:26.107587Z","iopub.status.idle":"2024-07-30T00:04:41.058639Z","shell.execute_reply.started":"2024-07-30T00:04:26.107547Z","shell.execute_reply":"2024-07-30T00:04:41.057628Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=b2ed54d031b197d982d42bf36e3bc0f0728999af0b2e632356ae059ec5d62399\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_metric\n\nrouge = load_metric(\"rouge\",trust_remote_code=True)\n\ndef compute_metrics(pred):\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions.argmax(-1)\n\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    labels_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n    \n    rouge_output = rouge.compute(predictions=pred_str, references=labels_str)\n    \n    return {\n        \"rouge1\": rouge_output[\"rouge1\"]*100,\n        \"rouge2\": rouge_output[\"rouge2\"]*100,\n        \"rougeL\": rouge_output[\"rougeL\"]*100,\n    }","metadata":{"execution":{"iopub.status.busy":"2024-07-30T00:04:41.062995Z","iopub.execute_input":"2024-07-30T00:04:41.063360Z","iopub.status.idle":"2024-07-30T00:04:42.779561Z","shell.execute_reply.started":"2024-07-30T00:04:41.063329Z","shell.execute_reply":"2024-07-30T00:04:42.778486Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/1212848942.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n  rouge = load_metric(\"rouge\",trust_remote_code=True)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cee41b49db2741f499d126bc41a49afd"}},"metadata":{}}]},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=new_model,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=1,\n    evaluation_strategy=\"no\",\n#     eval_steps=250,\n    logging_steps=100,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    weight_decay=0.01,\n    save_total_limit=2,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\",\n    push_to_hub=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T00:04:42.780935Z","iopub.execute_input":"2024-07-30T00:04:42.781381Z","iopub.status.idle":"2024-07-30T00:04:42.814972Z","shell.execute_reply.started":"2024-07-30T00:04:42.781336Z","shell.execute_reply":"2024-07-30T00:04:42.814107Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Function to perform manual garbage collection\ndef perform_gc():\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-07-30T00:04:42.816165Z","iopub.execute_input":"2024-07-30T00:04:42.816459Z","iopub.status.idle":"2024-07-30T00:04:42.822711Z","shell.execute_reply.started":"2024-07-30T00:04:42.816435Z","shell.execute_reply":"2024-07-30T00:04:42.821799Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    peft_config=peft_config,\n    max_seq_length=256,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False\n)\n# # Evaluate the model before training\n# results_before_training = trainer.evaluate()\n# print(\"Results before training:\", results_before_training)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T00:04:42.823857Z","iopub.execute_input":"2024-07-30T00:04:42.824192Z","iopub.status.idle":"2024-07-30T00:04:43.079864Z","shell.execute_reply.started":"2024-07-30T00:04:42.824167Z","shell.execute_reply":"2024-07-30T00:04:43.078927Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# torch.cuda.empty_cache()\n# perform_gc()","metadata":{"execution":{"iopub.status.busy":"2024-07-30T00:04:43.081047Z","iopub.execute_input":"2024-07-30T00:04:43.081406Z","iopub.status.idle":"2024-07-30T00:04:43.088604Z","shell.execute_reply.started":"2024-07-30T00:04:43.081371Z","shell.execute_reply":"2024-07-30T00:04:43.087544Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-30T00:04:43.090168Z","iopub.execute_input":"2024-07-30T00:04:43.090564Z","iopub.status.idle":"2024-07-30T03:17:01.571203Z","shell.execute_reply.started":"2024-07-30T00:04:43.090532Z","shell.execute_reply":"2024-07-30T03:17:01.570283Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4000/4000 3:10:38, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>4.791500</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>3.893000</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>3.961000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>3.987400</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>3.929300</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>3.703900</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>3.622700</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>3.632800</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>3.787400</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>3.822100</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>3.869500</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>3.593000</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>3.544400</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>3.682600</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>3.645300</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>3.473300</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>3.611800</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>3.588900</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>3.436000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>3.601300</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>3.431900</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>3.670200</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>3.503800</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>3.469400</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>3.257400</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>3.323800</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>3.350100</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>3.392400</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>3.491100</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>3.285400</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>3.444500</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>3.178500</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>3.265400</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>3.515700</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>3.373700</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>3.286700</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>3.343000</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>3.131600</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>3.246400</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>3.242700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=4000, training_loss=3.5595247497558593, metrics={'train_runtime': 11441.5589, 'train_samples_per_second': 0.35, 'train_steps_per_second': 0.35, 'total_flos': 9.2736011108352e+16, 'train_loss': 3.5595247497558593, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"# # Evaluate the model after training\n# results_after_training = trainer.evaluate()\n# print(\"Results after training:\", results_after_training)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T03:17:01.572795Z","iopub.execute_input":"2024-07-30T03:17:01.573085Z","iopub.status.idle":"2024-07-30T03:17:01.578156Z","shell.execute_reply.started":"2024-07-30T03:17:01.573057Z","shell.execute_reply":"2024-07-30T03:17:01.577039Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"wandb.finish()\n# perform_gc()\nmodel.config.use_cache = True","metadata":{"execution":{"iopub.status.busy":"2024-07-30T03:23:14.729484Z","iopub.execute_input":"2024-07-30T03:23:14.730381Z","iopub.status.idle":"2024-07-30T03:23:14.734669Z","shell.execute_reply.started":"2024-07-30T03:23:14.730348Z","shell.execute_reply":"2024-07-30T03:23:14.733730Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import wandb\nimport json\nimport os\n\n# Define the path to the run directory\nrun_dir = \"/kaggle/working/wandb/latest-run/files\"\nsummary_path = os.path.join(run_dir, 'wandb-summary.json')\n\n# Load the summary JSON file\nwith open(summary_path, 'r') as f:\n    summary = json.load(f)\n\n# Print the summary\nprint(summary)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T03:29:49.204614Z","iopub.execute_input":"2024-07-30T03:29:49.204992Z","iopub.status.idle":"2024-07-30T03:29:49.212934Z","shell.execute_reply.started":"2024-07-30T03:29:49.204963Z","shell.execute_reply":"2024-07-30T03:29:49.211835Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"{'train/loss': 3.2427, 'train/grad_norm': 6.079164028167725, 'train/learning_rate': 0.0, 'train/epoch': 1.0, 'train/global_step': 4000, '_timestamp': 1722309326.3270507, '_runtime': 11619.98102068901, '_step': 40, 'train_runtime': 11441.5589, 'train_samples_per_second': 0.35, 'train_steps_per_second': 0.35, 'total_flos': 9.2736011108352e+16, 'train_loss': 3.5595247497558593, '_wandb': {'runtime': 11713}}\n","output_type":"stream"}]},{"cell_type":"code","source":"new_model_name = \"llama-3-8b_fine_tuned\"\ntrainer.model.save_pretrained(new_model_name)\ntrainer.tokenizer.save_pretrained(new_model_name)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T03:23:26.169161Z","iopub.execute_input":"2024-07-30T03:23:26.169555Z","iopub.status.idle":"2024-07-30T03:23:31.892115Z","shell.execute_reply.started":"2024-07-30T03:23:26.169523Z","shell.execute_reply":"2024-07-30T03:23:31.891150Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"('llama-3-8b_fine_tuned/tokenizer_config.json',\n 'llama-3-8b_fine_tuned/special_tokens_map.json',\n 'llama-3-8b_fine_tuned/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"# torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-07-30T03:17:08.579171Z","iopub.status.idle":"2024-07-30T03:17:08.579538Z","shell.execute_reply.started":"2024-07-30T03:17:08.579362Z","shell.execute_reply":"2024-07-30T03:17:08.579377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}