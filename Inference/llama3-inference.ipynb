{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9067036,"sourceType":"datasetVersion","datasetId":5468597},{"sourceId":33547,"sourceType":"modelInstanceVersion","modelInstanceId":28079,"modelId":39106},{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083,"modelId":39106},{"sourceId":85257,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":71624,"modelId":96548}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-30T14:16:27.374791Z","iopub.execute_input":"2024-07-30T14:16:27.375170Z","iopub.status.idle":"2024-07-30T14:16:27.778817Z","shell.execute_reply.started":"2024-07-30T14:16:27.375141Z","shell.execute_reply":"2024-07-30T14:16:27.777772Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/llama3-finetuned-qs/transformers/default/1/adapter_model.safetensors\n/kaggle/input/llama3-finetuned-qs/transformers/default/1/adapter_config.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model.safetensors.index.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00003-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/config.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/LICENSE\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00001-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/USE_POLICY.md\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/tokenizer.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/tokenizer_config.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/example_text_completion.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/test_tokenizer.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/requirements.txt\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/tokenizer.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00004-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/eval_details.md\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/special_tokens_map.json\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/generation.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/model-00002-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/__init__.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/example_chat_completion.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/setup.py\n/kaggle/input/llama-3/transformers/8b-chat-hf/1/generation_config.json\n/kaggle/input/llama-3/transformers/8b-hf/1/model.safetensors.index.json\n/kaggle/input/llama-3/transformers/8b-hf/1/model-00003-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-hf/1/config.json\n/kaggle/input/llama-3/transformers/8b-hf/1/LICENSE\n/kaggle/input/llama-3/transformers/8b-hf/1/model-00001-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-hf/1/USE_POLICY.md\n/kaggle/input/llama-3/transformers/8b-hf/1/tokenizer.json\n/kaggle/input/llama-3/transformers/8b-hf/1/tokenizer_config.json\n/kaggle/input/llama-3/transformers/8b-hf/1/example_text_completion.py\n/kaggle/input/llama-3/transformers/8b-hf/1/requirements.txt\n/kaggle/input/llama-3/transformers/8b-hf/1/model-00004-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-hf/1/eval_details.md\n/kaggle/input/llama-3/transformers/8b-hf/1/special_tokens_map.json\n/kaggle/input/llama-3/transformers/8b-hf/1/model-00002-of-00004.safetensors\n/kaggle/input/llama-3/transformers/8b-hf/1/example_chat_completion.py\n/kaggle/input/llama-3/transformers/8b-hf/1/setup.py\n/kaggle/input/llama-3/transformers/8b-hf/1/generation_config.json\n/kaggle/input/quora-qs-ans/quora_dataset.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"%%capture\n%pip install -U bitsandbytes\n%pip install -U transformers\n%pip install -U accelerate\n%pip install -U peft\n%pip install -U trl","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:16:27.780893Z","iopub.execute_input":"2024-07-30T14:16:27.781290Z","iopub.status.idle":"2024-07-30T14:17:49.097185Z","shell.execute_reply.started":"2024-07-30T14:16:27.781262Z","shell.execute_reply":"2024-07-30T14:17:49.095947Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nlogin(token = hf_token)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:17:49.099199Z","iopub.execute_input":"2024-07-30T14:17:49.099502Z","iopub.status.idle":"2024-07-30T14:17:50.060824Z","shell.execute_reply.started":"2024-07-30T14:17:49.099474Z","shell.execute_reply":"2024-07-30T14:17:50.059947Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"base_model = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\nnew_model = \"/kaggle/input/llama3-finetuned-qs/transformers/default/1\"","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:17:50.062372Z","iopub.execute_input":"2024-07-30T14:17:50.062733Z","iopub.status.idle":"2024-07-30T14:17:50.066955Z","shell.execute_reply.started":"2024-07-30T14:17:50.062702Z","shell.execute_reply":"2024-07-30T14:17:50.066075Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\nfrom peft import PeftModel\nimport torch\nfrom trl import setup_chat_format","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:17:50.068960Z","iopub.execute_input":"2024-07-30T14:17:50.069251Z","iopub.status.idle":"2024-07-30T14:18:06.958973Z","shell.execute_reply.started":"2024-07-30T14:17:50.069227Z","shell.execute_reply":"2024-07-30T14:18:06.958068Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-07-30 14:17:56.363856: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-30 14:17:56.363979: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-30 14:17:56.496086: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Reload tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n\nbase_model_reload = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        return_dict=True,\n        low_cpu_mem_usage=True,\n        torch_dtype=torch.float16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:18:06.960268Z","iopub.execute_input":"2024-07-30T14:18:06.961012Z","iopub.status.idle":"2024-07-30T14:19:54.679117Z","shell.execute_reply.started":"2024-07-30T14:18:06.960975Z","shell.execute_reply":"2024-07-30T14:19:54.678280Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcd38eb923734a05b01d11e98b65f5e5"}},"metadata":{}}]},{"cell_type":"code","source":"# Merge adapter with base model\nbase_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)\nmodel = PeftModel.from_pretrained(base_model_reload, new_model)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:19:54.680317Z","iopub.execute_input":"2024-07-30T14:19:54.680629Z","iopub.status.idle":"2024-07-30T14:19:55.529494Z","shell.execute_reply.started":"2024-07-30T14:19:54.680583Z","shell.execute_reply":"2024-07-30T14:19:55.528515Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model = model.merge_and_unload()","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:19:55.530797Z","iopub.execute_input":"2024-07-30T14:19:55.531072Z","iopub.status.idle":"2024-07-30T14:19:55.758079Z","shell.execute_reply.started":"2024-07-30T14:19:55.531048Z","shell.execute_reply":"2024-07-30T14:19:55.757102Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# model.save_pretrained(\"llama-3-8b-chat-Qs-Ans-Quora\")\n# tokenizer.save_pretrained(\"llama-3-8b-Qs-Ans-Quora\")","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:20:01.529723Z","iopub.execute_input":"2024-07-30T14:20:01.530219Z","iopub.status.idle":"2024-07-30T14:20:01.534458Z","shell.execute_reply.started":"2024-07-30T14:20:01.530195Z","shell.execute_reply":"2024-07-30T14:20:01.533430Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# model.push_to_hub(\"llama-3-8b-chat-Qs-Ans-Quora\", use_temp_dir=False)\n# tokenizer.push_to_hub(\"llama-3-8b-chat-Qs-Ans-Quora\", use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:20:01.537992Z","iopub.execute_input":"2024-07-30T14:20:01.538507Z","iopub.status.idle":"2024-07-30T14:20:01.543608Z","shell.execute_reply.started":"2024-07-30T14:20:01.538481Z","shell.execute_reply":"2024-07-30T14:20:01.542764Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"dataset_path = '/kaggle/input/quora-qs-ans/quora_dataset.csv'\ndf = pd.read_csv(dataset_path)\ndf=df[:10]\n\n# Assuming your dataset has columns named 'qs' and 'ans'\nquestions = df['question'].tolist()\nreferences = df['answer'].tolist()\n\ngenerated_texts = []\nfor question in questions:\n    messages = [{\"role\": \"user\", \"content\": question}]\n    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    outputs = pipe(prompt, max_new_tokens=128, early_stopping=True, temperature=0.4, num_beams=3)\n    generated_text = outputs[0][\"generated_text\"]\n    generated_texts.append(generated_text)\n\ndf['generated'] = generated_texts\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:20:01.544878Z","iopub.execute_input":"2024-07-30T14:20:01.545538Z","iopub.status.idle":"2024-07-30T14:21:39.746757Z","shell.execute_reply.started":"2024-07-30T14:20:01.545514Z","shell.execute_reply":"2024-07-30T14:21:39.745752Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.4` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nYou seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"}]},{"cell_type":"code","source":"# from rouge_score import rouge_scorer\n\n# # Initialize the ROUGE scorer\n# scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n\n# # Calculate ROUGE scores for each pair of texts\n# scores = [scorer.score(ref, gen) for ref, gen in zip(df['ans'], df['generated'])]\n\n# # Convert the scores to a DataFrame for easier analysis\n# scores_df = pd.DataFrame(scores)\n\n# # Display the scores\n# import ace_tools as tools; tools.display_dataframe_to_user(name=\"ROUGE Scores\", dataframe=scores_df)\n# print(scores_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:21:39.747945Z","iopub.execute_input":"2024-07-30T14:21:39.748237Z","iopub.status.idle":"2024-07-30T14:21:39.752867Z","shell.execute_reply.started":"2024-07-30T14:21:39.748213Z","shell.execute_reply":"2024-07-30T14:21:39.751981Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:21:39.754105Z","iopub.execute_input":"2024-07-30T14:21:39.754452Z","iopub.status.idle":"2024-07-30T14:21:54.621290Z","shell.execute_reply.started":"2024-07-30T14:21:39.754404Z","shell.execute_reply":"2024-07-30T14:21:54.619960Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=c55c2307e69bede65db3c0ea25f0f54c83459cd16fea86d656de57ab379d1177\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from rouge_score import rouge_scorer\n\n# Function to compute ROUGE score\ndef compute_rouge(predictions, references):\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    scores = []\n    for pred, ref in zip(predictions, references):\n        score = scorer.score(ref, pred)\n        scores.append(score)\n    return scores\n\n# Compute ROUGE scores\nrouge_scores = compute_rouge(df['generated'], df['answer'])\n\n# Initialize variables to store total f-measure scores\ntotal_rouge1_fmeasure = 0\ntotal_rouge2_fmeasure = 0\ntotal_rougeL_fmeasure = 0\n\n# Sum the f-measure scores\nfor score in rouge_scores:\n    total_rouge1_fmeasure += score['rouge1'].fmeasure\n    total_rouge2_fmeasure += score['rouge2'].fmeasure\n    total_rougeL_fmeasure += score['rougeL'].fmeasure\n\n# Calculate average f-measure scores\nnum_samples = len(rouge_scores)\naverage_rouge1_fmeasure = total_rouge1_fmeasure / num_samples\naverage_rouge2_fmeasure = total_rouge2_fmeasure / num_samples\naverage_rougeL_fmeasure = total_rougeL_fmeasure / num_samples\n\n# Display the average f-measure scores\nprint(\"Average ROUGE Scores:\")\nprint(f\"ROUGE-1: {average_rouge1_fmeasure*100:.2f}\")\nprint(f\"ROUGE-2: {average_rouge2_fmeasure*100:.2f}\")\nprint(f\"ROUGE-L: {average_rougeL_fmeasure*100:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-30T14:25:37.346875Z","iopub.execute_input":"2024-07-30T14:25:37.347782Z","iopub.status.idle":"2024-07-30T14:25:37.882265Z","shell.execute_reply.started":"2024-07-30T14:25:37.347752Z","shell.execute_reply":"2024-07-30T14:25:37.881315Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Average ROUGE Scores:\nROUGE-1: 8.23\nROUGE-2: 1.60\nROUGE-L: 7.60\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}