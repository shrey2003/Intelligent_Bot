{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9064209,"sourceType":"datasetVersion","datasetId":3516822},{"sourceId":9060972,"sourceType":"datasetVersion","datasetId":5464240},{"sourceId":148861315,"sourceType":"kernelVersion"},{"sourceId":5112,"sourceType":"modelInstanceVersion","modelInstanceId":3900,"modelId":1902},{"sourceId":85054,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":71440,"modelId":96429}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# https://www.kaggle.com/code/hotchpotch/llm-detect-pip \n!pip install -q -U accelerate --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/","metadata":{"execution":{"iopub.status.busy":"2024-07-30T12:11:08.384234Z","iopub.execute_input":"2024-07-30T12:11:08.385299Z","iopub.status.idle":"2024-07-30T12:11:54.201052Z","shell.execute_reply.started":"2024-07-30T12:11:08.385252Z","shell.execute_reply":"2024-07-30T12:11:54.199576Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import sys\nimport torch\nimport random\nimport numpy as np\nimport pandas as pd\nimport gc\nfrom IPython.display import Markdown\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nsys.path.append(\"/kaggle/input/peft-main/src\")\nfrom peft import PeftModel\n\n#https://github.com/Lightning-AI/lit-gpt/issues/327\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n\nif (not torch.cuda.is_available()): print(\"Sorry - GPU required!\")","metadata":{"execution":{"iopub.status.busy":"2024-07-30T12:11:54.203380Z","iopub.execute_input":"2024-07-30T12:11:54.203751Z","iopub.status.idle":"2024-07-30T12:12:00.789503Z","shell.execute_reply.started":"2024-07-30T12:11:54.203719Z","shell.execute_reply":"2024-07-30T12:12:00.788607Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"model_name = '/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1'\n\n# Load base model(Mistral 7B)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=False,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    padding_side=\"left\",\n    add_eos_token=True,\n    add_bos_token=True,\n)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-07-30T12:12:00.790748Z","iopub.execute_input":"2024-07-30T12:12:00.791234Z","iopub.status.idle":"2024-07-30T12:13:41.203460Z","shell.execute_reply.started":"2024-07-30T12:12:00.791207Z","shell.execute_reply":"2024-07-30T12:13:41.202387Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1628591572ad4a1e9ccf06f1f3385006"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nYou are calling `save_pretrained` to a 4-bit converted model, but your `bitsandbytes` version doesn't support it. If you want to save 4-bit models, make sure to have `bitsandbytes>=0.41.3` installed.\n","output_type":"stream"}]},{"cell_type":"code","source":"def display_formatted(input_text):\n    input_text = input_text.replace(\"<s>\", \"\").replace(\"</s>\", \"\")\n    user_start = input_text.find(\"[INST]\") + len(\"[INST]\")\n    user_end = input_text.find(\"[/INST]\")\n    user_text = input_text[user_start:user_end].strip()\n    llm_response = input_text[user_end + len(\"[/INST]\"):].strip()\n    \n    formatted_text = f\"<b>User:</b><br>{user_text}\\n\\n<b>LLM Response:</b><br>{llm_response}\"\n\n    display(Markdown(formatted_text))\n    \ndef get_mistral_response(prompt):\n    # Construct the prompt\n    messages = [\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n    \n    # Apply the chat template and tokenize\n    model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n    model_inputs = model_inputs.to(\"cuda\")\n    \n    # Generate the response with stopping criteria\n    generated_ids = model.generate(\n        model_inputs, \n        max_new_tokens=128, \n        pad_token_id=tokenizer.eos_token_id,\n        eos_token_id=tokenizer.eos_token_id,  # Ensure end-of-sequence token is used\n        num_beams=4,\n        early_stopping=True\n    )\n\n    # Decode the generated ids\n    decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n    return decoded[0]","metadata":{"execution":{"iopub.status.busy":"2024-07-30T12:13:41.206271Z","iopub.execute_input":"2024-07-30T12:13:41.206995Z","iopub.status.idle":"2024-07-30T12:13:41.216735Z","shell.execute_reply.started":"2024-07-30T12:13:41.206958Z","shell.execute_reply":"2024-07-30T12:13:41.215694Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"response = get_mistral_response(\"What is the capital of France?\")\ndisplay_formatted(response)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T12:13:41.218024Z","iopub.execute_input":"2024-07-30T12:13:41.218313Z","iopub.status.idle":"2024-07-30T12:14:21.880974Z","shell.execute_reply.started":"2024-07-30T12:13:41.218288Z","shell.execute_reply":"2024-07-30T12:14:21.879882Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n2024-07-30 12:13:44.595535: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-30 12:13:44.595653: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-30 12:13:44.736881: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"<b>User:</b><br>What is the capital of France?\n\n<b>LLM Response:</b><br>The capital of France is Paris."},"metadata":{}}]},{"cell_type":"code","source":"model = PeftModel.from_pretrained(model, \"/kaggle/input/mistral_finetuned-quora/pytorch/default/1\")","metadata":{"execution":{"iopub.status.busy":"2024-07-30T12:14:21.882328Z","iopub.execute_input":"2024-07-30T12:14:21.882954Z","iopub.status.idle":"2024-07-30T12:14:27.832258Z","shell.execute_reply.started":"2024-07-30T12:14:21.882923Z","shell.execute_reply":"2024-07-30T12:14:27.831244Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"response = get_mistral_response(\"What is the capital of France?\")\ndisplay_formatted(response)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T12:14:27.833984Z","iopub.execute_input":"2024-07-30T12:14:27.834963Z","iopub.status.idle":"2024-07-30T12:14:48.699526Z","shell.execute_reply.started":"2024-07-30T12:14:27.834914Z","shell.execute_reply":"2024-07-30T12:14:48.698517Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"<b>User:</b><br>What is the capital of France?\n\n<b>LLM Response:</b><br>The capital city of France is Paris."},"metadata":{}}]},{"cell_type":"code","source":"response = get_mistral_response(\"Can you explain me theory of relativity\")\ndisplay_formatted(response)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T12:14:48.700773Z","iopub.execute_input":"2024-07-30T12:14:48.701088Z","iopub.status.idle":"2024-07-30T12:15:59.052442Z","shell.execute_reply.started":"2024-07-30T12:14:48.701062Z","shell.execute_reply":"2024-07-30T12:15:59.051429Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"<b>User:</b><br>Can you explain me theory of relativity\n\n<b>LLM Response:</b><br>Certainly! The theory of relativity is a fundamental concept in physics that explains the relationship between space and time. It was first proposed by Albert Einstein in the early 20th century and has since become one of the most well-established theories in modern physics.\nThere are two parts to the theory of relativity: the special theory of relativity and the general theory of relativity.\nThe special theory of relativity states that the laws of physics are the same for all observers moving at a constant speed in a straight line. It also states that the speed of light is always the same, regardless of the"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\n# Load your test dataset\ntest_data = pd.read_csv('/kaggle/input/mistral-qna/quora_dataset.csv')\n\n\n# Assuming your test data has columns 'source_text' and 'reference_summary'\nsource_texts = test_data['question'].tolist()\nreference_summaries = test_data['answer'].tolist()","metadata":{"execution":{"iopub.status.busy":"2024-07-30T13:24:43.436786Z","iopub.execute_input":"2024-07-30T13:24:43.437188Z","iopub.status.idle":"2024-07-30T13:24:44.002980Z","shell.execute_reply.started":"2024-07-30T13:24:43.437158Z","shell.execute_reply":"2024-07-30T13:24:44.002061Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"predictions = []\n\nfor text in source_texts:\n    response = get_mistral_response(text)\n    predictions.append(response)","metadata":{"execution":{"iopub.status.busy":"2024-07-30T13:24:44.249487Z","iopub.execute_input":"2024-07-30T13:24:44.249926Z","iopub.status.idle":"2024-07-30T13:35:38.107440Z","shell.execute_reply.started":"2024-07-30T13:24:44.249894Z","shell.execute_reply":"2024-07-30T13:35:38.106355Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"!pip install rouge-score","metadata":{"execution":{"iopub.status.busy":"2024-07-30T13:35:38.109151Z","iopub.execute_input":"2024-07-30T13:35:38.109471Z","iopub.status.idle":"2024-07-30T13:35:53.232365Z","shell.execute_reply.started":"2024-07-30T13:35:38.109445Z","shell.execute_reply":"2024-07-30T13:35:53.231366Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=cbdb1a3697aff3f4627757d1dc97b2956471f869ad6a2f1a1bc8f8f198718ca5\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score\nSuccessfully installed rouge-score-0.1.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from rouge_score import rouge_scorer\n\n# Function to compute ROUGE score\ndef compute_rouge(predictions, references):\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    scores = []\n    for pred, ref in zip(predictions, references):\n        score = scorer.score(ref, pred)\n        scores.append(score)\n    return scores\n\n# Compute ROUGE scores\nrouge_scores = compute_rouge(predictions, reference_summaries)\n\n# Extract fmeasure values and compute averages\naverage_rouge1 = sum([score['rouge1'].fmeasure for score in rouge_scores]) / len(rouge_scores)\naverage_rouge2 = sum([score['rouge2'].fmeasure for score in rouge_scores]) / len(rouge_scores)\naverage_rougeL = sum([score['rougeL'].fmeasure for score in rouge_scores]) / len(rouge_scores)\n\nprint(f'Average ROUGE-1: {average_rouge1*100}')\nprint(f'Average ROUGE-2: {average_rouge2*100}')\nprint(f'Average ROUGE-L: {average_rougeL*100}')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-30T13:38:52.125594Z","iopub.execute_input":"2024-07-30T13:38:52.126659Z","iopub.status.idle":"2024-07-30T13:38:52.205358Z","shell.execute_reply.started":"2024-07-30T13:38:52.126616Z","shell.execute_reply":"2024-07-30T13:38:52.204297Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Average ROUGE-1: 9.42831563783953\nAverage ROUGE-2: 1.6946171722881727\nAverage ROUGE-L: 7.541255808609577\n","output_type":"stream"}]},{"cell_type":"code","source":"model.push_to_hub(\"llama-3-8b-chat-Qs-Ans-Quora\", use_temp_dir=False)\ntokenizer.push_to_hub(\"llama-3-8b-chat-Qs-Ans-Quora\", use_temp_dir=False)","metadata":{},"execution_count":null,"outputs":[]}]}