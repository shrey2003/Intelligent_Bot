{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9060972,"sourceType":"datasetVersion","datasetId":5464240},{"sourceId":5111,"sourceType":"modelInstanceVersion","modelInstanceId":3899,"modelId":1902}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-29T19:16:57.325784Z","iopub.execute_input":"2024-07-29T19:16:57.326487Z","iopub.status.idle":"2024-07-29T19:16:58.430899Z","shell.execute_reply.started":"2024-07-29T19:16:57.326435Z","shell.execute_reply":"2024-07-29T19:16:58.429974Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/mistral-qna/quora_dataset.csv\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/config.json\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/pytorch_model-00002-of-00002.bin\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/tokenizer.json\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/tokenizer_config.json\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/pytorch_model.bin.index.json\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/pytorch_model-00001-of-00002.bin\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/special_tokens_map.json\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/.gitattributes\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/tokenizer.model\n/kaggle/input/mistral/pytorch/7b-v0.1-hf/1/generation_config.json\n","output_type":"stream"}]},{"cell_type":"code","source":"#complains a bunch about updated packages - seems to work fine\n%pip install -q -U bitsandbytes\n%pip install -q -U trl \n%pip install -q -U accelerate\n%pip install -q -U transformers\n%pip install -q -U peft\n%pip install -q datasets==2.16.0","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:16:58.432936Z","iopub.execute_input":"2024-07-29T19:16:58.433526Z","iopub.status.idle":"2024-07-29T19:18:32.971575Z","shell.execute_reply.started":"2024-07-29T19:16:58.433493Z","shell.execute_reply":"2024-07-29T19:18:32.970375Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.6.1 requires cubinlinker, which is not installed.\ncudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.6.1 requires ptxcompiler, which is not installed.\ncuml 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\ncudf 24.6.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.5.1 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\ngcsfs 2024.5.0 requires fsspec==2024.5.0, but you have fsspec 2023.10.0 which is incompatible.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\nrapids-dask-dependency 24.6.0a0 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\ns3fs 2024.5.0 requires fsspec==2024.5.0.*, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport torch, wandb\nimport numpy as np\nimport pandas as pd\nimport warnings\nfrom datasets import Dataset\nfrom IPython.display import Markdown, display\nfrom IPython.utils import io\n\nfrom peft import PeftModel, LoraConfig, prepare_model_for_kbit_training, get_peft_model\nfrom transformers import BitsAndBytesConfig, AutoTokenizer, TrainingArguments, AutoModelForCausalLM, HfArgumentParser, TrainingArguments, pipeline, logging\nfrom trl import SFTTrainer\n","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:18:32.973163Z","iopub.execute_input":"2024-07-29T19:18:32.973460Z","iopub.status.idle":"2024-07-29T19:18:51.839779Z","shell.execute_reply.started":"2024-07-29T19:18:32.973432Z","shell.execute_reply":"2024-07-29T19:18:51.838949Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-07-29 19:18:41.350426: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-29 19:18:41.350585: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-29 19:18:41.499448: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n\nmodel_path = \"/kaggle/input/mistral/pytorch/7b-v0.1-hf/1\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n    bnb_4bit_use_double_quant=False,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_path,\n    quantization_config=bnb_config,\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\nmodel.config.use_cache = False # silence the warnings. Please re-enable for inference!\nmodel.config.pretraining_tp = 1\nmodel.gradient_checkpointing_enable()\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\ntokenizer.padding_side = 'right'\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_eos_token = True\ntokenizer.add_bos_token, tokenizer.add_eos_token\n","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:18:51.841816Z","iopub.execute_input":"2024-07-29T19:18:51.842416Z","iopub.status.idle":"2024-07-29T19:21:34.296872Z","shell.execute_reply.started":"2024-07-29T19:18:51.842388Z","shell.execute_reply":"2024-07-29T19:21:34.295848Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2c8ff896f4e4d1cb9ec5182d1cb5e19"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(True, True)"},"metadata":{}}]},{"cell_type":"code","source":"#Adding the adapters in the layers\nmodel = prepare_model_for_kbit_training(model)\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:21:34.298160Z","iopub.execute_input":"2024-07-29T19:21:34.298454Z","iopub.status.idle":"2024-07-29T19:21:35.797747Z","shell.execute_reply.started":"2024-07-29T19:21:34.298424Z","shell.execute_reply":"2024-07-29T19:21:35.796964Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def display_formatted(input_text):\n    input_text = input_text.replace(\"<s>\", \"\").replace(\"</s>\", \"\")\n    user_start = input_text.find(\"[INST]\") + len(\"[INST]\")\n    user_end = input_text.find(\"[/INST]\")\n    user_text = input_text[user_start:user_end].strip()\n    llm_response = input_text[user_end + len(\"[/INST]\"):].strip()\n    \n    formatted_text = f\"<b>User:</b><br>{user_text}\\n\\n<b>LLM Response:</b><br>{llm_response}\"\n\n    display(Markdown(formatted_text))\n\n\ndef get_mistral_response(prompt):\n    # Construct the prompt\n    messages = [\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n    \n    # Apply the chat template and tokenize\n    model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n    model_inputs = model_inputs.to(\"cuda\")\n    \n    # Generate the response with stopping criteria\n    generated_ids = model.generate(\n        model_inputs, \n        max_new_tokens=350, \n        pad_token_id=tokenizer.eos_token_id,\n        eos_token_id=tokenizer.eos_token_id,  # Ensure end-of-sequence token is used\n        early_stopping=True\n    )\n\n    # Decode the generated ids\n    decoded = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n    return decoded[0]","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:21:35.798953Z","iopub.execute_input":"2024-07-29T19:21:35.799267Z","iopub.status.idle":"2024-07-29T19:21:35.807280Z","shell.execute_reply.started":"2024-07-29T19:21:35.799241Z","shell.execute_reply":"2024-07-29T19:21:35.806283Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# Load the train and test data\ndf = pd.read_csv('/kaggle/input/mistral-qna/quora_dataset.csv')\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\ntest_df=test_df[:250]\ntrain_df=train_df[:4000]\n\ntrain_df['question'] = train_df['question'].astype(str)\ntrain_df['answer'] = train_df['answer'].astype(str)\ntest_df['question'] = test_df['question'].astype(str)\ntest_df['answer'] = test_df['answer'].astype(str)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:21:35.808370Z","iopub.execute_input":"2024-07-29T19:21:35.808725Z","iopub.status.idle":"2024-07-29T19:21:36.833055Z","shell.execute_reply.started":"2024-07-29T19:21:35.808701Z","shell.execute_reply":"2024-07-29T19:21:36.832267Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\n\ndef prepare_dataset(df):\n    chats = []\n    for index, row in df.iterrows():\n        chat = [\n            {\"role\": \"user\", \"content\": row['question']},\n            {\"role\": \"assistant\", \"content\": row['answer']},\n        ]\n        chats.append(chat)\n    \n    chat_dataset = Dataset.from_dict({\"chat\": chats})\n    \n    formatted_chat_data = chat_dataset.map(\n        lambda x: {\"formatted_chat\": tokenizer.apply_chat_template(x[\"chat\"], tokenize=False, add_generation_prompt=False)}\n    )\n    \n    return formatted_chat_data\n\ntrain_dataset = prepare_dataset(train_df)\ntest_dataset = prepare_dataset(test_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:21:36.834205Z","iopub.execute_input":"2024-07-29T19:21:36.834558Z","iopub.status.idle":"2024-07-29T19:21:38.131034Z","shell.execute_reply.started":"2024-07-29T19:21:36.834527Z","shell.execute_reply":"2024-07-29T19:21:38.130036Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"853933df980742108386d6ae76af4689"}},"metadata":{}},{"name":"stderr","text":"No chat template is set for this tokenizer, falling back to a default class-level template. This is very error-prone, because models are often trained with templates different from the class default! Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98ae197063ce4526bf87dd1113585e09"}},"metadata":{}}]},{"cell_type":"code","source":"pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:21:38.132280Z","iopub.execute_input":"2024-07-29T19:21:38.132596Z","iopub.status.idle":"2024-07-29T19:21:52.672088Z","shell.execute_reply.started":"2024-07-29T19:21:38.132552Z","shell.execute_reply":"2024-07-29T19:21:52.670898Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=983f9f7102b9b0968f100345e03c68458dbd10a0115fca4c7d914d436e150fec\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n\nlogin(token = hf_token)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:21:52.676638Z","iopub.execute_input":"2024-07-29T19:21:52.676971Z","iopub.status.idle":"2024-07-29T19:21:52.991818Z","shell.execute_reply.started":"2024-07-29T19:21:52.676943Z","shell.execute_reply":"2024-07-29T19:21:52.990884Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"import wandb\nwb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\nwandb.init(project=\"mistral_fine_tune\",\n    job_type=\"training\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:21:52.993056Z","iopub.execute_input":"2024-07-29T19:21:52.993869Z","iopub.status.idle":"2024-07-29T19:22:12.058700Z","shell.execute_reply.started":"2024-07-29T19:21:52.993835Z","shell.execute_reply":"2024-07-29T19:22:12.057722Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrajeevkhanna31167\u001b[0m (\u001b[33mpersonal_007\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.5 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240729_192155-g55lypc9</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/personal_007/mistral_fine_tune/runs/g55lypc9' target=\"_blank\">vague-frost-6</a></strong> to <a href='https://wandb.ai/personal_007/mistral_fine_tune' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/personal_007/mistral_fine_tune' target=\"_blank\">https://wandb.ai/personal_007/mistral_fine_tune</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/personal_007/mistral_fine_tune/runs/g55lypc9' target=\"_blank\">https://wandb.ai/personal_007/mistral_fine_tune/runs/g55lypc9</a>"},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/personal_007/mistral_fine_tune/runs/g55lypc9?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7b71f7d90c70>"},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_metric\n\nrouge = load_metric(\"rouge\")\n\ndef compute_metrics(pred):\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions.argmax(-1)\n\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    labels_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n    \n    rouge_output = rouge.compute(predictions=pred_str, references=labels_str)\n    \n    return {\n        \"rouge1\": rouge_output[\"rouge1\"]*100,\n        \"rouge2\": rouge_output[\"rouge2\"]*100,\n        \"rougeL\": rouge_output[\"rougeL\"]*100,\n    }\n","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:22:12.060201Z","iopub.execute_input":"2024-07-29T19:22:12.060485Z","iopub.status.idle":"2024-07-29T19:22:13.034167Z","shell.execute_reply.started":"2024-07-29T19:22:12.060461Z","shell.execute_reply":"2024-07-29T19:22:13.033167Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/3440905356.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n  rouge = load_metric(\"rouge\")\n/opt/conda/lib/python3.10/site-packages/datasets/load.py:752: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.0/metrics/rouge/rouge.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3472f4bc55944bc3a2511acc0d35b6f0"}},"metadata":{}}]},{"cell_type":"code","source":"# Training arguments with reduced batch size and gradient checkpointing\ntraining_arguments = TrainingArguments(\n    output_dir='./mistral_fine_tune',\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=1,\n    evaluation_strategy=\"steps\",\n    eval_steps=200,  # Adjust the steps for evaluation as needed\n    logging_steps=100,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=True,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\",\n    dataloader_pin_memory=True,  # Optimize data loading\n    dataloader_num_workers=4,  # Optimize data loading\n    gradient_checkpointing=True,  # Enable gradient checkpointing\n\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:22:13.035504Z","iopub.execute_input":"2024-07-29T19:22:13.035816Z","iopub.status.idle":"2024-07-29T19:22:13.071127Z","shell.execute_reply.started":"2024-07-29T19:22:13.035790Z","shell.execute_reply":"2024-07-29T19:22:13.070062Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# Function to perform manual garbage collection\ndef perform_gc():\n    gc.collect()\n    torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:22:13.073055Z","iopub.execute_input":"2024-07-29T19:22:13.073433Z","iopub.status.idle":"2024-07-29T19:22:13.080752Z","shell.execute_reply.started":"2024-07-29T19:22:13.073397Z","shell.execute_reply":"2024-07-29T19:22:13.079938Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    peft_config=peft_config,\n    max_seq_length=256,\n    dataset_text_field=\"formatted_chat\",\n    args=training_arguments,\n    packing=False,\n#     compute_metrics=compute_metrics\n)\n# results_before_training = trainer.evaluate()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:22:13.081785Z","iopub.execute_input":"2024-07-29T19:22:13.082077Z","iopub.status.idle":"2024-07-29T19:22:15.024606Z","shell.execute_reply.started":"2024-07-29T19:22:13.082054Z","shell.execute_reply":"2024-07-29T19:22:15.023600Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b784be5abe9403da57646802d3900cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a936832b19bd4e00b362ecc489c8e582"}},"metadata":{}}]},{"cell_type":"code","source":"# torch.cuda.empty_cache()\n# perform_gc()","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:22:15.025820Z","iopub.execute_input":"2024-07-29T19:22:15.026123Z","iopub.status.idle":"2024-07-29T19:22:15.030851Z","shell.execute_reply.started":"2024-07-29T19:22:15.026097Z","shell.execute_reply":"2024-07-29T19:22:15.029871Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"trainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:22:15.032041Z","iopub.execute_input":"2024-07-29T19:22:15.032314Z","iopub.status.idle":"2024-07-29T22:59:49.431595Z","shell.execute_reply.started":"2024-07-29T19:22:15.032290Z","shell.execute_reply":"2024-07-29T22:59:49.430581Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2000/2000 3:37:25, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>1.357300</td>\n      <td>1.355378</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.207300</td>\n      <td>1.312295</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.267800</td>\n      <td>1.273623</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.229100</td>\n      <td>1.239618</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.221100</td>\n      <td>1.218344</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.241600</td>\n      <td>1.203349</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>1.127700</td>\n      <td>1.189386</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>1.103900</td>\n      <td>1.173675</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>1.150100</td>\n      <td>1.165976</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.141600</td>\n      <td>1.162935</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=2000, training_loss=1.2275247268676759, metrics={'train_runtime': 13052.5893, 'train_samples_per_second': 0.306, 'train_steps_per_second': 0.153, 'total_flos': 3.611466506067149e+16, 'train_loss': 1.2275247268676759, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"# # Evaluate the model after training\n# perform_gc()\n# results_after_training = trainer.evaluate()\n# print(\"Results after training:\", results_after_training)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" trainer.model.push_to_hub('mistral_fine_tune', use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T23:02:09.101154Z","iopub.execute_input":"2024-07-29T23:02:09.101868Z","iopub.status.idle":"2024-07-29T23:03:05.284192Z","shell.execute_reply.started":"2024-07-29T23:02:09.101834Z","shell.execute_reply":"2024-07-29T23:03:05.283056Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/369M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"147a92ab4bf94e359e583c907afe662a"}},"metadata":{}},{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Feluda/mistral_fine_tune/commit/130efb92dd84f7a3d0cc38768e6269e9c5e7537b', commit_message='Upload model', commit_description='', oid='130efb92dd84f7a3d0cc38768e6269e9c5e7537b', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":" trainer.tokenizer.push_to_hub('mistral_fine_tune', use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T23:10:01.311376Z","iopub.execute_input":"2024-07-29T23:10:01.312105Z","iopub.status.idle":"2024-07-29T23:10:33.611127Z","shell.execute_reply.started":"2024-07-29T23:10:01.312070Z","shell.execute_reply":"2024-07-29T23:10:33.610213Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e72467706c44fd99e4d88e1e3f5b04a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86060cf9bda74fcfb97e2f134a97d9b1"}},"metadata":{}},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Feluda/mistral_fine_tune/commit/3740d70015233e7ec33523b28d793e51a3502edd', commit_message='Upload tokenizer', commit_description='', oid='3740d70015233e7ec33523b28d793e51a3502edd', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"new_model_name = \"mistral-7b_fine_tuned\"\ntrainer.model.save_pretrained(new_model_name)\ntrainer.tokenizer.save_pretrained(new_model_name)\nwandb.finish()\nmodel.config.use_cache = True","metadata":{"execution":{"iopub.status.busy":"2024-07-29T23:04:59.151831Z","iopub.execute_input":"2024-07-29T23:04:59.152496Z","iopub.status.idle":"2024-07-29T23:05:03.829715Z","shell.execute_reply.started":"2024-07-29T23:04:59.152464Z","shell.execute_reply":"2024-07-29T23:05:03.829026Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▆▅▄▃▂▂▁▁▁</td></tr><tr><td>eval/runtime</td><td>▄▂█▇▃▃▁▃▂▂</td></tr><tr><td>eval/samples_per_second</td><td>██▁▁██████</td></tr><tr><td>eval/steps_per_second</td><td>██▁▁██████</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>▂▄█▄▃▄▂▄▄▄▄▄▆▃▇▃▃▆▃▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▅▄▃▄▃▃▃▃▃▃▃▁▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.16294</td></tr><tr><td>eval/runtime</td><td>229.5767</td></tr><tr><td>eval/samples_per_second</td><td>1.089</td></tr><tr><td>eval/steps_per_second</td><td>1.089</td></tr><tr><td>total_flos</td><td>3.611466506067149e+16</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>2000</td></tr><tr><td>train/grad_norm</td><td>0.24484</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.1416</td></tr><tr><td>train_loss</td><td>1.22752</td></tr><tr><td>train_runtime</td><td>13052.5893</td></tr><tr><td>train_samples_per_second</td><td>0.306</td></tr><tr><td>train_steps_per_second</td><td>0.153</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">vague-frost-6</strong> at: <a href='https://wandb.ai/personal_007/mistral_fine_tune/runs/g55lypc9' target=\"_blank\">https://wandb.ai/personal_007/mistral_fine_tune/runs/g55lypc9</a><br/> View project at: <a href='https://wandb.ai/personal_007/mistral_fine_tune' target=\"_blank\">https://wandb.ai/personal_007/mistral_fine_tune</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240729_192155-g55lypc9/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}